{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01aee92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T18:56:10.166038Z",
     "start_time": "2024-03-14T18:56:10.161911Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import iris\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tobac\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import netCDF4 as nc \n",
    "import warnings\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "# Ignore some warnings and append them to the existing filter list\n",
    "warnings.filterwarnings('ignore', category=UserWarning, append=True)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, append=True)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, append=True)\n",
    "warnings.filterwarnings('ignore',category=pd.io.pytables.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5527c67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T18:56:11.139523Z",
     "start_time": "2024-03-14T18:56:10.564805Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the 'caffeine' module to prevent the system from going to sleep or the screen from turning off\n",
    "import caffeine\n",
    "# Turn on the caffeine mode with the display option set to True\n",
    "caffeine.on(display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a088c410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T18:56:11.264085Z",
     "start_time": "2024-03-14T18:56:11.255963Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a function for initial processing of an ISCCP HXG file\n",
    "def transforming_nc(datafile):\n",
    "    '''\n",
    "    This function processes a NetCDF file containing IR brightness temperature data.\n",
    "    It calculates converted brightness temperatures, adds them to an xarray dataset, \n",
    "    removes unneeded variables, and returns the processed dataset \n",
    "    with converted temperature values.\n",
    "    Input: A function takes a directory to a file as an argument\n",
    "    Output: Processed dataset\n",
    "    '''\n",
    "    file = nc.Dataset(datafile)\n",
    "    irad = np.array(file['irad']) # irad is a calibrated IR brightness temperature in standard counts\n",
    "    tmbtab = np.array(file['tmptab']) # tmptab is a count to temperature conversion table\n",
    "    TB = tmbtab[irad] # Converting brightness temperature to Kelvin\n",
    "    vtauic = np.array(file['vtauic']) # vtauic is an all cloud optical thicknesses retrieved for both liquid and ice phase\n",
    "    tautab = np.array(file['tautab']) # tautab is a count to optical thickness conversion table\n",
    "    TAU = tautab[vtauic]\n",
    "    ds = xr.open_dataset(datafile)\n",
    "    temp = xr.DataArray(TB,dims = [\"lat\", \"lon\"]) # Creating xarray from converted TB values\n",
    "    tau = xr.DataArray(TAU, dims = [\"lat\", \"lon\"]) # Creating xarray from converted TAU values\n",
    "    ds['Tb'] = temp # Adding TB values as a variable to the original dataset\n",
    "    ds['Tau'] = tau # Adding TAU values as a variable to the original dataset\n",
    "    dataset_keys = list(ds.keys()) # A list of all the variables\n",
    "    dataset_keys.remove('Tb')\n",
    "    dataset_keys.remove('Tau')\n",
    "    dataset_keys.remove('time')\n",
    "    ds = ds.drop(labels=dataset_keys) # Removing all the variables that are not needed\n",
    "    ds = ds.set_coords('time')\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab37df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T18:56:12.436523Z",
     "start_time": "2024-03-14T18:56:12.381057Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specify the directory path and the file format you want to filter\n",
    "#path = \"DATA/\"\n",
    "path = \"/Volumes/Pegasus32 R8/NASA/RAW_DATA/2010\"\n",
    "file_format = \"*.nc\" \n",
    "# Use glob to get the list of files matching the specified format\n",
    "file_list = glob(os.path.join(path, file_format))\n",
    "# Now file_list contains only the files of the specified format in the specified directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91779699",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T18:56:13.239344Z",
     "start_time": "2024-03-14T18:56:13.235542Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the length of the list\n",
    "print(len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7269b25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T18:56:13.991454Z",
     "start_time": "2024-03-14T18:56:13.989072Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sort the values of the list\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb41565",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T18:56:18.415151Z",
     "start_time": "2024-03-14T18:56:18.411651Z"
    }
   },
   "outputs": [],
   "source": [
    "file_list = file_list[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec82d13f",
   "metadata": {},
   "source": [
    "# The cell below is used for the processing of the year 2009 only as one of the files is corrupted and needs to be processed in a sligtly different way than the rest of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c20b36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T15:20:01.085586Z",
     "start_time": "2023-11-16T15:12:20.491903Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_datasets = []\n",
    "for f in file_list:\n",
    "    print(f)\n",
    "    if f == '/Volumes/Pegasus32 R8/NASA/RAW_DATA/2009/ISCCPHXG.v01r00.GLOBAL.2009.08.05.1800.GPC.10KM.CS0952909559.EQ0.10.nc':\n",
    "        # This particular file is missing lon, lat, time and tmptab information\n",
    "        # Since this information is the same for all the file, I'm going to replace missing values \n",
    "        # with the data from another file\n",
    "        # Loading non corrupted dataset and variables from it\n",
    "        non_corrupted  = xr.open_dataset('/Volumes/Pegasus32 R8/NASA/RAW_DATA/2009/ISCCPHXG.v01r00.GLOBAL.2009.08.05.2100.GPC.10KM.CS0952909559.EQ0.10.nc')\n",
    "        lons = non_corrupted.lon.values\n",
    "        lats = non_corrupted.lat.values\n",
    "        correct_tmptab = non_corrupted.tmptab.values\n",
    "        correct_tautab = non_corrupted.tautab.values\n",
    "        # Defining variable with the correct file\n",
    "        time = '2009-08-05T18:00:00.000000000'\n",
    "        # Loading corrupted file\n",
    "        corrupted = xr.open_dataset('/Volumes/Pegasus32 R8/NASA/RAW_DATA/2009/ISCCPHXG.v01r00.GLOBAL.2009.08.05.1800.GPC.10KM.CS0952909559.EQ0.10.nc')\n",
    "        # Replace latitude and longitude coordinates in the existing xarray dataset\n",
    "        corrupted = corrupted.assign_coords(lat=lats, lon=lons)\n",
    "        # Specify the attribute names and values in a dictionary\n",
    "        attributes_dict_lon = {\n",
    "            \"long_name\": \"Center longitude of square grid cell\",\n",
    "            \"units\": \"degrees_east\",\n",
    "            \"valid_min\": \"0.0\",\n",
    "            \"valid_max\":\"360.0\",\n",
    "            \"bounds\":\"lon_bounds\"\n",
    "        }\n",
    "        attributes_dict_lat = {\n",
    "            \"long_name\": \"Center latitude of square grid cell\",\n",
    "            \"units\": \"degrees_north\",\n",
    "            \"valid_min\": \"-90.0\",\n",
    "            \"valid_max\":\"90.0\",\n",
    "            \"bounds\":\"lat_bounds\"\n",
    "        }\n",
    "        # Add the attributes to the variable\n",
    "        for attribute_name, attribute_value in attributes_dict_lon.items():\n",
    "            corrupted['lon'].attrs[attribute_name] = attribute_value\n",
    "        for attribute_name, attribute_value in attributes_dict_lat.items():\n",
    "            corrupted['lat'].attrs[attribute_name] = attribute_value\n",
    "        # Assigning correct time \n",
    "        corrupted['time'].values = time\n",
    "        corrupted[\"time\"] = corrupted[\"time\"].astype('datetime64[ns]')\n",
    "        # Assigning correct tmptab values\n",
    "        corrupted['tmptab'].values = correct_tmptab\n",
    "        # Assigning correct tautab values\n",
    "        corrupted['tautab'].values = correct_tautab\n",
    "        # Next processing the file as regular\n",
    "        file = nc.Dataset('/Volumes/Pegasus32 R8/NASA/RAW_DATA/2009/ISCCPHXG.v01r00.GLOBAL.2009.08.05.1800.GPC.10KM.CS0952909559.EQ0.10.nc')\n",
    "        irad = np.array(file['irad']) # irad is a calibrated IR brightness temperature in standard counts\n",
    "        vtauic = np.array(file['vtauic'])\n",
    "        tmptab = np.array(corrupted['tmptab']) # tmptab is a count to temperature conversion table\n",
    "        tautab = np.array(corrupted['tmptab']) \n",
    "        TB = tmptab[irad] # Converting brightness temperature to Kelvin\n",
    "        TAU = tautab[vtauic]\n",
    "        temp = xr.DataArray(TB,dims=[\"lat\", \"lon\"]) # Creating a xarray from converted TB values\n",
    "        tau = xr.DataArray(TAU,dims=[\"lat\", \"lon\"])\n",
    "        corrupted['Tb'] = temp # Adding TB values as a variable to the original dataset\n",
    "        corrupted['Tau'] = tau\n",
    "        dataset_keys = list(corrupted.keys()) # A list of all the variables\n",
    "        dataset_keys.remove('Tb')\n",
    "        dataset_keys.remove('Tau')\n",
    "        dataset_keys.remove('time')\n",
    "        corrupted = corrupted.drop(labels=dataset_keys) # Removing all the variables that are not needed\n",
    "        corrupted = corrupted.set_coords('time')\n",
    "        processed_datasets.append(corrupted)\n",
    "    else:\n",
    "        dataset = transforming_nc(f)\n",
    "        processed_datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959fd983",
   "metadata": {},
   "source": [
    "# Regular processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da3c99e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T18:56:39.898532Z",
     "start_time": "2024-03-14T18:56:21.699498Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process each file in the file_list using the transforming_nc function\n",
    "processed_datasets = [transforming_nc(file) for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9224b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T18:58:01.417742Z",
     "start_time": "2024-03-14T18:58:00.614892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming 'datasets' is a list containing xarray datasets with dimensions 'time', 'lat', and 'lon'\n",
    "\n",
    "# Chunk each dataset along the 'time' dimension\n",
    "chunked_datasets = [dataset.chunk({'time': -1}) for dataset in processed_datasets]\n",
    "\n",
    "# Concatenate the chunked datasets\n",
    "concatenated_dataset = xr.concat(chunked_datasets, dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c864701",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T18:57:34.663142Z",
     "start_time": "2024-03-14T18:57:34.651542Z"
    }
   },
   "outputs": [],
   "source": [
    "del file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49fbd46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T18:57:40.248182Z",
     "start_time": "2024-03-14T18:57:37.720942Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate a list of processed datasets along the \"time\" dimension\n",
    "merged_dataset = xr.concat(processed_datasets, dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd84b16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T18:57:45.059191Z",
     "start_time": "2024-03-14T18:57:45.040225Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c75b5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T16:00:27.956483Z",
     "start_time": "2024-03-14T16:00:27.886770Z"
    }
   },
   "outputs": [],
   "source": [
    "del processed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66859d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T16:45:25.451954Z",
     "start_time": "2024-03-14T16:45:25.434838Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a0315c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T15:59:43.809776Z",
     "start_time": "2024-03-14T15:59:43.804639Z"
    }
   },
   "outputs": [],
   "source": [
    "def transforming_file(file):\n",
    "    '''\n",
    "    The function futher transforms dataset that contains brightness temperature data\n",
    "    to make the description of all variables more clear and concise.\n",
    "    Output: Iris cube as it is required as an input in tobac processing\n",
    "    '''\n",
    "    # Define latitude range for data subset\n",
    "    min_lat = -60\n",
    "    max_lat = 60\n",
    "    # Create a mask to extract data within the specified latitude range\n",
    "    subset_mask = (file.lat >= min_lat) & (file.lat <= max_lat)\n",
    "    # Apply the latitude subset mask and create a subset dataset\n",
    "    subset_ds = file.where(subset_mask, drop=True)\n",
    "    # Update time attributes\n",
    "    subset_ds.time.attrs['axis'] = 'T'\n",
    "    subset_ds.time.attrs['standard_name'] = 'time'\n",
    "    del subset_ds.time.attrs['long_name']\n",
    "    # Update longitude attributes\n",
    "    subset_ds.lon.attrs['axis'] = 'X'\n",
    "    subset_ds.lon.attrs['units'] = 'degrees_east'\n",
    "    subset_ds.lon.attrs['standard_name'] = 'longitude'\n",
    "    subset_ds.lon.attrs['spacing'] = '0.1'\n",
    "    # Update latitude attributes\n",
    "    subset_ds.lat.attrs['axis'] = 'Y'\n",
    "    subset_ds.lat.attrs['units'] = 'degrees_north'\n",
    "    subset_ds.lat.attrs['standard_name'] = 'latitude'\n",
    "    subset_ds.lat.attrs['spacing'] = '0.1'\n",
    "    # Update Tb (brightness temperature) attributes\n",
    "    subset_ds.Tb.attrs['long_name'] = 'Tb'\n",
    "    subset_ds.Tb.attrs['units'] = 'K'\n",
    "    # Save information for both TB and TAU\n",
    "    TAU_TB = subset_ds\n",
    "    # Remove TAU as it is not needed in tobac processing\n",
    "    subset_ds = subset_ds.drop_vars('Tau')\n",
    "    # Convert subset_ds.Tb to an Iris cube\n",
    "    TB = subset_ds.Tb.to_iris()\n",
    "    # Return the processed Iris cube and xarray dataset\n",
    "    return TB, TAU_TB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7d712a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T15:23:36.823454Z",
     "start_time": "2024-03-14T15:23:36.820504Z"
    }
   },
   "outputs": [],
   "source": [
    "#Set up directory to save output and plots:\n",
    "savedir='Save'\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)\n",
    "plot_dir=\"Plot\"\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8691ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T15:23:40.274989Z",
     "start_time": "2024-03-14T15:23:39.861710Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace -1000 with NaN\n",
    "merged_dataset = merged_dataset.where(merged_dataset != -1000, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544d8100",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T15:34:12.982274Z",
     "start_time": "2024-03-14T15:34:12.218315Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform the merged dataset using the transforming_file function\n",
    "TB, TAU_TB = transforming_file(merged_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fee7f3a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T00:14:42.267Z"
    }
   },
   "outputs": [],
   "source": [
    "del merged_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d28c8d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T00:14:42.762Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save both TB and TAU information\n",
    "iris.save([TB],os.path.join(savedir,'TB.nc'),zlib=True,complevel=4)\n",
    "TAU_TB.to_netcdf('Save/TAU_TB.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c07fb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T00:14:43.643Z"
    }
   },
   "outputs": [],
   "source": [
    "del TAU_TB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686e42c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T15:34:15.354709Z",
     "start_time": "2024-03-14T15:34:15.351641Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate spatial and temporal spacings using the 'get_spacings' function from the 'tobac' library\n",
    "dxy, dt = tobac.get_spacings(TB, grid_spacing=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1bfd9b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T00:14:44.383Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keyword arguments for the feature detection step\n",
    "parameters_features=dict()\n",
    "parameters_features['target']='minimum'\n",
    "parameters_features['threshold']= [245,220]  \n",
    "parameters_features['n_min_threshold'] = 2  # The higher the number, the larger features will get detected\n",
    "parameters_features['position_threshold']= 'weighted_diff'\n",
    "parameters_features['sigma_threshold']=1.5 # The larger the values, the fewer features detected\n",
    "parameters_features['n_erosion_threshold']=2 # The larger the values, the fewer features detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe26aaa",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T00:14:45.275Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature detection and save results to file:\n",
    "print('starting feature detection')\n",
    "Features=tobac.feature_detection_multithreshold(TB,dxy,**parameters_features)\n",
    "Features.to_hdf(os.path.join(savedir,'Features.h5'),'table')\n",
    "print('feature detection performed and saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7771d0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T00:14:45.649Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "# Create a count plot \n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "ax = sns.countplot(x='threshold_value', data=Features)\n",
    "# Customize plot labels and title\n",
    "plt.xlabel(\"Threshold Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Threshold Values\")\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f56f20c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T00:14:46.034Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keyword arguments for the segmentation step:\n",
    "parameters_segmentation={}\n",
    "parameters_segmentation['target']='minimum' \n",
    "parameters_segmentation['threshold']=245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ccb463",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T00:14:47.142Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform segmentation and save results to files:\n",
    "Mask_TB,Features_TB=tobac.segmentation_2D(Features,TB,dxy,**parameters_segmentation)\n",
    "print('segmentation TB performed, start saving results to files')\n",
    "iris.save([Mask_TB],os.path.join(savedir,'Mask_Segmentation_TB.nc'),zlib=True,complevel=4)                \n",
    "Features_TB.to_hdf(os.path.join(savedir,'Features_TB.h5'),'table')\n",
    "print('segmentation TB performed and saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55a8115",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T00:14:47.543Z"
    }
   },
   "outputs": [],
   "source": [
    "del Features_TB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04046c87",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T00:14:47.920Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keyword arguments for linking step:\n",
    "parameters_linking={}\n",
    "parameters_linking['method_linking']='predict' \n",
    "parameters_linking['v_max']=30 #(m/s)Assumed maximum speed of tracked objects\n",
    "parameters_linking['adaptive_stop']=2 # Tells trackpy when to give up\n",
    "parameters_linking['adaptive_step']=0.95 # Can only be in range 0-1\n",
    "parameters_linking['stubs']=2 #Minumum number of timesteps for which objects have to be      \n",
    "                                #detected to not be filtered out as spurious\n",
    "parameters_linking['subnetwork_size']=20 \n",
    "parameters_linking['time_cell_min']=5*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b854ba",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T00:14:48.891Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform linking and save results to file:\n",
    "Track=tobac.linking_trackpy(Features,TB,dt=dt,dxy=dxy,**parameters_linking)\n",
    "Track.to_hdf(os.path.join(savedir,'Track.h5'),'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d6e00",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T00:14:49.825Z"
    }
   },
   "outputs": [],
   "source": [
    "Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d2619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
